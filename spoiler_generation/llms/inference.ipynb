{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please run\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "bin .env/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda113.so\n",
      "CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 8.0\n",
      "CUDA SETUP: Detected CUDA version 113\n",
      "CUDA SETUP: Loading binary .env/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda113.so...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ".env/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: /opt/conda did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...\n",
      "  warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from peft import PeftModel, PeftConfig\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overriding torch_dtype=None with `torch_dtype=torch.float16` due to requirements of `bitsandbytes` to enable model loading in mixed int8. Either pass torch_dtype=torch.float16 or don't pass this argument at all to remove this warning.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "992836070c774fddb112aff2b1f853be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "peft_model_id = \"models/llama-13b-without-type\"\n",
    "config = PeftConfig.from_pretrained(peft_model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(config.base_model_name_or_path, return_dict=True, load_in_8bit=True, device_map={\"\": 0})\n",
    "tokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path, padding_side=\"left\", model_max_length=1024)\n",
    "\n",
    "# Load the Lora model\n",
    "model = PeftModel.from_pretrained(model, peft_model_id, device_map={\"\": 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.add_special_tokens({\"pad_token\": tokenizer.bos_token})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('This simple household item saves lives. What household item saves lives?',\n",
       " 'Erin Zaikis often used to think about how she could help change the world, but it wasn\\'t until she got very sick with dengue fever that she decided to take action. In high school, Zaikis\\' mother was diagnosed with breast cancer, and the teen slipped into depression -- trapped in what she described as a cycle of feeling sorry for herself. In college, she took a course in global poverty, which helped put her own struggles in perspective. And at 19, the Boston native traveled to Mumbai, where she spent a summer living in an orphanage that housed around 100 girls, some of whom had been left in trash cans or abandoned in train stations. \"I\\'m from a middle-, upper-class town and had never been exposed to extreme poverty, or starvation, or issues like child trafficking,\" Zaikis said. \"It was life-changing.\" After graduating, Zaikis, now 24, traveled to Thailand, where she worked with organizations that fight child trafficking. She visited schools to assess children\\'s risk of being trafficked and to match them with local vocational programs. During one such visit, she watched as middle school-age children entered and left a bathroom without washing their hands. With the help of a translator, she asked the children if they had soap, and was startled to learn they didn\\'t know what soap was. The closest shop that sold it was a several-hour walk away. Zaikis made the trek and bought roughly 150 bars of pink and blue soap, which cost her $30 U.S. \"Cigarettes were much more expensive, but cigarettes were bought by almost everyone in the community,\" Zaikis said. \"I really think the problem did not lie in the cost of soap, but rather the education. No one has spent the time to take these children aside and teach them how to wash their hands.\" Zaikis knew her soap purchase was a temporary fix, and made plans to spend the summer partnering with schools and local organizations on hygiene projects. But within days, she collapsed and was rushed to the hospital with dengue hemorrhagic fever -- an often deadly infection spread by mosquitos. Her parents took her back to Boston, where she was treated at Massachusetts General Hospital. \"It was a really horrible couple of months,\" Zaikis said. \"But I think I really needed that because I realized, one, life is short. And two, I could have died of dengue fever if I had stayed in this rural area. ... I felt incredibly fortunate to have survived, and realized I really wanted to dedicate my life to helping other people.\" Once Zaikis recovered, she took a computer coding class and began searching for soap recipes. \"The first soap burnt all my bowls and dishes,\" she laughed. \"I set off the fire alarm. I cut my hand five times.\" She slowly perfected the process as well as the ingredients, testing her concoctions on herself, then finally on friends and family. Roughly one month ago, Zaikis launched the website for Sundara, where she sells three handmade soaps, each inspired by a different country, and each shaped like a flower. The Thailand soap is scented with lemongrass and pomelo, and a dollar from the sale of each soap supports the Hug Project\\'s mission of funding medical professionals who schedule health visits with \"street boys\" and children at risk of being trafficked, as well as providing them with hygiene kits. Proceeds from the lavender and shea butter soap go to Ghana, where the Unlock Foundation is funding the construction of a sink and well in one school and supplying soap to the area, while proceeds from the chai tea soap go toward Gabriel Project Mumbai, which trains women in the community to lead hygiene workshops in schools located in local slums. The goal, Zaikis said, is to help educate children about hygiene, and to give them the tools to protect themselves -- an uphill climb. The United Nations estimates that some 2,000 children younger than 5 die every day from diarrheal diseases, and the majority of those deaths are tied to water, sanitation and hygiene.. Zaikis works on Sundara full time, making all of the soaps and their packaging by hand. The business helps her pay rent on the Manhattan apartment she shares with her sister, but Zaikis frequently takes babysitting and dog-sitting jobs to supplement her income. She used money she\\'d saved while living in Thailand to buy soap ingredients -- things like spirulina and kelp -- which now crowd her apartment. Her personal favorite is the chai soap. \"Whenever I use it in the shower, I start thinking about tea, and I think, \\'Oh, this smells edible!\\'\" she said. \"It\\'s just a fun one to smell.\" Zaikis plans to expand Sundara and to begin working with refugee women in New York to teach them how to make the soap. The happiness she gets from her work and from seeing photos of completed projects funded by her donations, she said, is unlike anything else. \"Every day I ask myself, what am I doing for other people?\" she said. \"This soap company is a great way to have fun, and learn about business, but more importantly, to help other people. ... It\\'s a way to serve in some small way, and help someone who needs it.\" Zaikis with her soaps, which she makes by hand in her New York City apartment.',\n",
       " 'phrase')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_json(\"data/llama_generation/test.json\")\n",
    "data = list(zip(df[\"question\"], df[\"context\"], df[\"type\"]))\n",
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT = {\n",
    "    \"phrase\": (\n",
    "        \"Below is an question paired with an context for which generate answer.\"\n",
    "        \"Write a answer as short as possible (max 5 words). Use only words from context.\\n\\n\"\n",
    "        \"### Question:\\n{question}\\n\\n### Context:\\n{context}\\n\\n### Answer:\\n\"\n",
    "    ),\n",
    "    \"passage\": (\n",
    "        \"Below is an question paired with an context for which generate answer.\"\n",
    "        \"Write a answer which will be from one to three sentences. Use only words from context.\\n\\n\"\n",
    "        \"### Question:\\n{question}\\n\\n### Context:\\n{context}\\n\\n### Answer:\\n\"\n",
    "    ),\n",
    "    \"multi\": (\n",
    "        \"Below is an question paired with an context for which generate answer.\"\n",
    "        \"Write a answer which is multi part that means it contains multiple \"\n",
    "        \"phrase or sentences from given text. Use only words from context.\\n\\n\"\n",
    "        \"### Question:\\n{question}\\n\\n### Context:\\n{context}\\n\\n### Answer:\\n\"\n",
    "    ),\n",
    "}\n",
    "# PROMPT = (\n",
    "#         \"Below is an question paired with an context for which generate answer. \"\n",
    "#         \"Write a answer that with type {type} appropriately completes question.\\n\\n\"\n",
    "#         \"### Question:\\n{question}\\n\\n### Context:\\n{context}\\n\\n### Answer:\\n\"\n",
    "#     )\n",
    "input_ids = tokenizer(\n",
    "    [\n",
    "        PROMPT[typ].format(\n",
    "            question=clickbait,\n",
    "            context=context,\n",
    "        )\n",
    "        for clickbait, context, typ in data[1:3]\n",
    "    ],\n",
    "    return_tensors=\"pt\",\n",
    "    max_length=1912,\n",
    "    padding=\"max_length\",\n",
    "    truncation=True,\n",
    ").to(\"cuda:0\")\n",
    "input_ids.pop(\"token_type_ids\")\n",
    "generated_ids = model.generate(**input_ids, max_new_tokens=50, do_sample=True, num_return_sequences=3)\n",
    "\n",
    "batch = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "soap\n",
      "phrase\n"
     ]
    }
   ],
   "source": [
    "print(df.loc[0, \"output\"])\n",
    "print(df.loc[0, \"type\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:00<00:00, 18369.21it/s]\n",
      "100%|██████████| 6/6 [00:14<00:00,  2.44s/it]t]\n",
      "100%|██████████| 6/6 [00:00<00:00, 54120.05it/s]]\n",
      "100%|██████████| 6/6 [00:00<00:00, 49441.70it/s]]\n",
      "100%|██████████| 6/6 [00:14<00:00,  2.40s/it]/it]\n",
      "100%|██████████| 6/6 [00:13<00:00,  2.25s/it]/it]\n",
      "100%|██████████| 6/6 [00:19<00:00,  3.27s/it]/it]\n",
      "100%|██████████| 6/6 [00:00<00:00, 51781.53it/s]]\n",
      "100%|██████████| 6/6 [00:18<00:00,  3.14s/it]/it]\n",
      "100%|██████████| 6/6 [00:13<00:00,  2.29s/it]/it]\n",
      "100%|██████████| 6/6 [00:16<00:00,  2.74s/it]s/it]\n",
      "100%|██████████| 6/6 [00:00<00:00, 48865.68it/s]t]\n",
      "100%|██████████| 6/6 [00:00<00:00, 30174.85it/s]t]\n",
      "100%|██████████| 6/6 [00:00<00:00, 50533.78it/s]t]\n",
      "100%|██████████| 6/6 [00:00<00:00, 52428.80it/s]t]\n",
      "100%|██████████| 6/6 [00:00<00:00, 15611.55it/s]t]\n",
      "100%|██████████| 6/6 [00:13<00:00,  2.29s/it]it]  \n",
      "100%|██████████| 6/6 [00:00<00:00, 18222.90it/s]t]\n",
      "100%|██████████| 6/6 [00:00<00:00, 45507.82it/s]t]\n",
      "100%|██████████| 6/6 [00:00<00:00, 40590.04it/s]  \n",
      "100%|██████████| 6/6 [00:00<00:00, 17722.41it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 46175.82it/s]\n",
      "100%|██████████| 6/6 [00:16<00:00,  2.79s/it]it]\n",
      "100%|██████████| 6/6 [00:00<00:00, 50131.12it/s]t]\n",
      "100%|██████████| 6/6 [00:00<00:00, 46175.82it/s]  \n",
      "100%|██████████| 6/6 [00:14<00:00,  2.47s/it]it]\n",
      "100%|██████████| 6/6 [00:00<00:00, 52869.38it/s]t]\n",
      "100%|██████████| 6/6 [00:42<00:00,  7.13s/it]it]  \n",
      "100%|██████████| 6/6 [00:13<00:00,  2.30s/it]s/it]\n",
      "100%|██████████| 6/6 [00:00<00:00, 41053.55it/s]t]\n",
      "100%|██████████| 6/6 [00:14<00:00,  2.34s/it]s/it]\n",
      "100%|██████████| 6/6 [00:00<00:00, 50031.46it/s]t]\n",
      "100%|██████████| 6/6 [00:00<00:00, 18545.19it/s]t]\n",
      "100%|██████████| 6/6 [00:27<00:00,  4.59s/it]s/it]\n",
      "100%|██████████| 6/6 [00:00<00:00, 47482.69it/s]t]\n",
      "100%|██████████| 6/6 [00:00<00:00, 48960.75it/s]t]\n",
      "100%|██████████| 6/6 [00:00<00:00, 23323.28it/s]  \n",
      "100%|██████████| 6/6 [00:00<00:00, 44150.57it/s]\n",
      "100%|██████████| 6/6 [00:13<00:00,  2.28s/it]it]\n",
      "100%|██████████| 6/6 [00:13<00:00,  2.29s/it]it]\n",
      "100%|██████████| 6/6 [00:00<00:00, 27563.88it/s]t]\n",
      "100%|██████████| 6/6 [00:00<00:00, 47393.27it/s]  \n",
      "100%|██████████| 6/6 [00:00<00:00, 53204.70it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 48865.68it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 53773.13it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 50840.05it/s]\n",
      "100%|██████████| 6/6 [00:18<00:00,  3.14s/it]it]\n",
      "100%|██████████| 6/6 [00:00<00:00, 50231.19it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 54120.05it/s]\n",
      "100%|██████████| 6/6 [00:13<00:00,  2.27s/it]it]\n",
      "100%|██████████| 6/6 [00:00<00:00, 50331.65it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 40787.40it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 42013.06it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 45181.01it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 43018.50it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 42581.77it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 37560.93it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 38421.11it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 25216.26it/s]\n",
      "100%|██████████| 6/6 [00:32<00:00,  5.38s/it]it]\n",
      "100%|██████████| 6/6 [00:00<00:00, 49734.83it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 18355.82it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 33825.03it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 52428.80it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 17524.95it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 48489.06it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 47215.43it/s]\n",
      "100%|██████████| 6/6 [00:14<00:00,  2.38s/it]it]\n",
      "100%|██████████| 6/6 [00:12<00:00,  2.10s/it]it]\n",
      "100%|██████████| 6/6 [00:00<00:00, 49932.19it/s]\n",
      "100%|██████████| 6/6 [00:13<00:00,  2.20s/it]it]\n",
      "100%|██████████| 6/6 [00:00<00:00, 48865.68it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 52869.38it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 38304.15it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 47572.45it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 48302.93it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 43389.35it/s]\n",
      "100%|██████████| 6/6 [00:13<00:00,  2.28s/it]it]\n",
      "100%|██████████| 6/6 [00:00<00:00, 39945.75it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 52980.68it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 47572.45it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 46689.84it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 29606.85it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 49056.19it/s]\n",
      "100%|██████████| 6/6 [00:16<00:00,  2.78s/it]it]\n",
      "100%|██████████| 6/6 [00:12<00:00,  2.13s/it]it]\n",
      "100%|██████████| 6/6 [00:00<00:00, 19152.07it/s]\n",
      "100%|██████████| 6/6 [00:13<00:00,  2.28s/it]it]\n",
      "100%|██████████| 6/6 [00:18<00:00,  3.06s/it]it]\n",
      "100%|██████████| 6/6 [00:00<00:00, 50737.55it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 42871.93it/s]\n",
      "100%|██████████| 6/6 [00:13<00:00,  2.22s/it]it]\n",
      "100%|██████████| 6/6 [00:00<00:00, 57195.05it/s]\n",
      "100%|██████████| 6/6 [00:27<00:00,  4.63s/it]it]\n",
      "100%|██████████| 6/6 [00:15<00:00,  2.59s/it]it]\n",
      "100%|██████████| 6/6 [00:00<00:00, 45019.36it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 41459.35it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 5222.21it/s]]\n",
      "100%|██████████| 6/6 [00:00<00:00, 50942.96it/s]\n",
      "100%|██████████| 6/6 [00:13<00:00,  2.18s/it]it]\n",
      "100%|██████████| 6/6 [00:00<00:00, 42509.84it/s]]\n",
      "100%|██████████| 6/6 [00:00<00:00, 45923.04it/s]]\n",
      "100%|██████████| 6/6 [00:00<00:00, 52428.80it/s]]\n",
      "100%|██████████| 6/6 [00:12<00:00,  2.08s/it]/it]\n",
      "100%|██████████| 6/6 [00:31<00:00,  5.28s/it]/it]\n",
      "100%|██████████| 6/6 [00:12<00:00,  2.12s/it]/it]\n",
      "100%|██████████| 6/6 [00:00<00:00, 28024.30it/s]]\n",
      "100%|██████████| 6/6 [00:13<00:00,  2.20s/it]/it]\n",
      "100%|██████████| 6/6 [00:00<00:00, 51358.82it/s]]\n",
      "100%|██████████| 6/6 [00:00<00:00, 43314.67it/s]]\n",
      "100%|██████████| 6/6 [00:15<00:00,  2.55s/it]/it]\n",
      "100%|██████████| 6/6 [00:12<00:00,  2.10s/it]/it]\n",
      "100%|██████████| 6/6 [00:00<00:00, 51675.20it/s]]\n",
      "100%|██████████| 6/6 [00:12<00:00,  2.14s/it]/it]\n",
      "100%|██████████| 6/6 [00:15<00:00,  2.62s/it]/it]\n",
      "100%|██████████| 6/6 [00:13<00:00,  2.20s/it]/it]\n",
      "100%|██████████| 6/6 [00:00<00:00, 43166.08it/s]]\n",
      "100%|██████████| 6/6 [00:00<00:00, 48302.93it/s]]\n",
      "100%|██████████| 6/6 [00:00<00:00, 38479.85it/s]]\n",
      "100%|██████████| 6/6 [00:00<00:00, 47127.01it/s]]\n",
      "100%|██████████| 6/6 [00:00<00:00, 54003.91it/s]]\n",
      "100%|██████████| 6/6 [00:12<00:00,  2.11s/it]/it]\n",
      "100%|██████████| 6/6 [00:00<00:00, 51150.05it/s]]\n",
      "100%|██████████| 6/6 [00:00<00:00, 55188.21it/s]]\n",
      "100%|██████████| 6/6 [00:15<00:00,  2.63s/it]/it]\n",
      "100%|██████████| 6/6 [00:12<00:00,  2.10s/it]/it]\n",
      "100%|██████████| 6/6 [00:00<00:00, 49636.73it/s]]\n",
      "100%|██████████| 6/6 [00:13<00:00,  2.23s/it]/it]\n",
      "100%|██████████| 6/6 [00:00<00:00, 43018.50it/s]]\n",
      "100%|██████████| 6/6 [00:00<00:00, 56552.41it/s]]\n",
      "100%|██████████| 6/6 [00:00<00:00, 56807.73it/s]]\n",
      "100%|██████████| 6/6 [00:00<00:00, 52211.25it/s]]\n",
      "100%|██████████| 6/6 [00:00<00:00, 57719.78it/s]]\n",
      "100%|██████████| 6/6 [00:00<00:00, 58936.36it/s]]\n",
      "100%|██████████| 6/6 [00:16<00:00,  2.81s/it]/it]\n",
      "100%|██████████| 6/6 [00:00<00:00, 58936.36it/s]]\n",
      "100%|██████████| 6/6 [00:00<00:00, 53317.42it/s]]\n",
      "100%|██████████| 6/6 [00:00<00:00, 29676.68it/s]]\n",
      "100%|██████████| 6/6 [00:00<00:00, 52319.80it/s]]\n",
      "100%|██████████| 6/6 [00:00<00:00, 48489.06it/s]]\n",
      "100%|██████████| 6/6 [00:00<00:00, 5185.62it/s]/it]\n",
      "100%|██████████| 6/6 [00:12<00:00,  2.12s/it]0s/it]\n",
      "100%|██████████| 6/6 [00:00<00:00, 51995.50it/s]it]\n",
      "100%|██████████| 6/6 [00:00<00:00, 56679.78it/s]it]\n",
      "100%|██████████| 6/6 [00:13<00:00,  2.17s/it]1s/it]\n",
      "100%|██████████| 6/6 [00:00<00:00, 32640.50it/s]it]\n",
      "100%|██████████| 6/6 [00:12<00:00,  2.12s/it]6s/it]\n",
      "100%|██████████| 6/6 [00:00<00:00, 19313.76it/s]it]\n",
      "100%|██████████| 6/6 [00:12<00:00,  2.12s/it]4s/it]\n",
      "100%|██████████| 6/6 [00:00<00:00, 50331.65it/s]it]\n",
      "100%|██████████| 6/6 [00:00<00:00, 54471.48it/s]it]\n",
      "100%|██████████| 6/6 [00:00<00:00, 48865.68it/s]it]\n",
      "100%|██████████| 6/6 [00:18<00:00,  3.08s/it]6s/it]\n",
      "100%|██████████| 6/6 [00:00<00:00, 56048.61it/s]it]\n",
      "100%|██████████| 6/6 [00:00<00:00, 37560.93it/s]it]\n",
      "100%|██████████| 6/6 [00:00<00:00, 42295.50it/s]it]\n",
      "100%|██████████| 6/6 [00:00<00:00, 54827.50it/s]it]\n",
      "100%|██████████| 6/6 [00:17<00:00,  2.91s/it]7s/it]\n",
      "100%|██████████| 6/6 [00:00<00:00, 57456.22it/s]it]\n",
      "100%|██████████| 6/6 [00:14<00:00,  2.48s/it]1s/it]\n",
      "100%|██████████| 6/6 [00:13<00:00,  2.18s/it]1s/it]\n",
      "100%|██████████| 6/6 [00:00<00:00, 50331.65it/s]it]\n",
      "100%|██████████| 6/6 [00:00<00:00, 52980.68it/s]it]\n",
      "100%|██████████| 6/6 [00:00<00:00, 51150.05it/s]it]\n",
      "100%|██████████| 6/6 [00:17<00:00,  2.98s/it]7s/it]\n",
      "100%|██████████| 6/6 [00:12<00:00,  2.14s/it]8s/it]\n",
      "100%|██████████| 4/4 [00:00<00:00, 15520.09it/s]it]\n",
      "100%|██████████| 167/167 [1:11:23<00:00, 25.65s/it]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "# PROMPT = {\n",
    "#     \"phrase\": (\n",
    "#         \"Below is an question paired with an context for which generate answer.\"\n",
    "#         \"Write a answer as short as possible (max 5 words). Use only words from context.\\n\\n\"\n",
    "#         \"### Question:\\n{question}\\n\\n### Context:\\n{context}\\n\\n### Answer:\\n\"\n",
    "#     ),\n",
    "#     \"passage\": (\n",
    "#         \"Below is an question paired with an context for which generate answer.\"\n",
    "#         \"Write a answer which will be from one to three sentences. Use only words from context.\\n\\n\"\n",
    "#         \"### Question:\\n{question}\\n\\n### Context:\\n{context}\\n\\n### Answer:\\n\"\n",
    "#     ),\n",
    "#     \"multi\": (\n",
    "#         \"Below is an question paired with an context for which generate answer.\"\n",
    "#         \"Write a answer which is multi part that means it contains multiple \"\n",
    "#         \"phrase or sentences from given text. Use only words from context.\\n\\n\"\n",
    "#         \"### Question:\\n{question}\\n\\n### Context:\\n{context}\\n\\n### Answer:\\n\"\n",
    "#     ),\n",
    "# }\n",
    "PROMPT = (\n",
    "    \"Below is an question paired with an context for which generate answer. \"\n",
    "    \"Write a answer that appropriately completes question.\\n\\n\"\n",
    "    \"### Question:\\n{question}\\n\\n### Context:\\n{context}\\n\\n### Answer:\\n\"\n",
    ")\n",
    "\n",
    "# PROMPT = (\n",
    "#         \"Below is an question paired with an context for which generate answer. \"\n",
    "#         \"Write a answer that with type {type} appropriately completes question.\\n\\n\"\n",
    "#         \"### Question:\\n{question}\\n\\n### Context:\\n{context}\\n\\n### Answer:\\n\"\n",
    "#     )\n",
    "REGRESSOR_PROMPT = \"For given question:\\n {} \\nanswer:\\n {} \\ncontext:\\n{}\"\n",
    "spoilers_generated = []\n",
    "batch_size = 6\n",
    "j = 0\n",
    "\n",
    "for i in tqdm(range(len(data) // batch_size + 1)):\n",
    "    data_batch = data[i * batch_size : (i + 1) * batch_size]\n",
    "    input_ids = tokenizer(\n",
    "        [PROMPT.format(question=clickbait, context=context) for clickbait, context, typ in data_batch],\n",
    "        return_tensors=\"pt\",\n",
    "        max_length=1912,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "    ).to(\"cuda:0\")\n",
    "    input_ids.pop(\"token_type_ids\")\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        generated_ids = model.generate(**input_ids, max_new_tokens=50)\n",
    "\n",
    "    spoilers_batch = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "    for spoiler in tqdm(spoilers_batch):\n",
    "        generated = spoiler.split(\"Answer:\\n\")\n",
    "        if len(generated) == 1:\n",
    "            input_ids = tokenizer(\n",
    "                [PROMPT.format(question=clickbait, context=context[:4500]) for clickbait, context, typ in data[j : j + 1]],\n",
    "                return_tensors=\"pt\",\n",
    "                max_length=2048,\n",
    "                padding=\"max_length\",\n",
    "                truncation=True,\n",
    "            ).to(\"cuda:0\")\n",
    "            input_ids.pop(\"token_type_ids\")\n",
    "\n",
    "            with torch.inference_mode():\n",
    "                generated_ids = model.generate(**input_ids, max_new_tokens=50)\n",
    "\n",
    "            spoilers_batch = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "            generated = spoilers_batch[0].split(\"Answer:\\n\")\n",
    "        generated = generated[1]\n",
    "\n",
    "        if data[j][2] == \"multi\":\n",
    "            generated = generated.replace(\"</s>\", \"\\n\")\n",
    "        else:\n",
    "            generated = generated.split(\"</s>\")[0]\n",
    "\n",
    "        spoilers_generated.append(generated)\n",
    "        j += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(spoilers_generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.DataFrame(zip(range(1000), spoilers_generated), columns=[\"id\", \"spoiler\"]).to_csv(\n",
    "    \"models/llama-13b-without-type/output.csv\", index=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>spoiler</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>soap</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Gwyneth Paltrow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>They're being so darn romantic together, it's ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>JaVale McGee\\ninspired to sponsor an Amateur A...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Alex Owens-Sarno</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>995</td>\n",
       "      <td>‘All Along the Watchtower.’ The Jimi Hendrix c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>996</td>\n",
       "      <td>1. Catch it on a good day.\\n2. Smooth it out.\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>997</td>\n",
       "      <td>full lunar eclipse\\nvisible in the United Stat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>998</td>\n",
       "      <td>Tami Erin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>999</td>\n",
       "      <td>celery</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      id                                            spoiler\n",
       "0      0                                               soap\n",
       "1      1                                    Gwyneth Paltrow\n",
       "2      2  They're being so darn romantic together, it's ...\n",
       "3      3  JaVale McGee\\ninspired to sponsor an Amateur A...\n",
       "4      4                                   Alex Owens-Sarno\n",
       "..   ...                                                ...\n",
       "995  995  ‘All Along the Watchtower.’ The Jimi Hendrix c...\n",
       "996  996  1. Catch it on a good day.\\n2. Smooth it out.\\...\n",
       "997  997  full lunar eclipse\\nvisible in the United Stat...\n",
       "998  998                                          Tami Erin\n",
       "999  999                                             celery\n",
       "\n",
       "[1000 rows x 2 columns]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv(\"models/llama-13b-finetuned/output.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
