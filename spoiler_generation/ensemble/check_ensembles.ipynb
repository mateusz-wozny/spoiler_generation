{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spoiler generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from evaluate import load\n",
    "import numpy as np\n",
    "import itertools\n",
    "\n",
    "from spoiler_generation.utils.stats import prepare_stats\n",
    "from spoiler_generation.utils.dataset_class import Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The dataset is private\n",
    "dataset = load_dataset(\"MateuszW/clickbait_spoiling_test\")\n",
    "test_df = pd.DataFrame(dataset[\"test\"])\n",
    "test_df[\"spoiler\"] = test_df[\"spoiler\"].apply(Dataset.preprocess_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 6)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset csv (C:/Users/Mateusz/.cache/huggingface/datasets/MateuszW___csv/MateuszW--spoiler_generation-2d60d0350a7a6926/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e631ad4eca24fd3af52d599f0a37883",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "models_outputs = load_dataset(\n",
    "    \"MateuszW/spoiler_generation\",\n",
    "    data_files={\n",
    "        \"baseline\": \"models_output/deberta-baseline_output.csv\",\n",
    "        \"deberta_paqott\": \"models_output/deberta-paqott_output.csv\",\n",
    "        \"llama_pwot\": \"models_output/llama-pwot_output.csv\",\n",
    "        \"vicuna_pwot\": \"models_output/vicuna-pwot_output.csv\",\n",
    "        \"opt_pwot\": \"models_output/opt-pwot_output.csv\",\n",
    "        \"llama_pwt\": \"models_output/llama-pwt_output.csv\",\n",
    "        \"vicuna_pwt\": \"models_output/vicuna-pwt_output.csv\",\n",
    "        \"opt_pwt\": \"models_output/opt-pwt_output.csv\",\n",
    "        \"vicuna_ppt\": \"models_output/vicuna-ppt_output.csv\",\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "for output_name in models_outputs.keys():\n",
    "    models_outputs[output_name] = pd.DataFrame(models_outputs[output_name])\n",
    "    models_outputs[output_name][\"spoiler\"] = models_outputs[output_name][\"spoiler\"].apply(Dataset.preprocess_func)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regressor"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"MateuszW/regressor-deberta-iter1-iter2\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_path, num_labels=1).to(\"cuda\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT = \"For given question:\\n {} \\nanswer:\\n {} \\ncontext:\\n{}\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select spoilers without using models for specific type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = pd.DataFrame(\n",
    "    zip(\n",
    "        models_outputs[\"llama_pwt\"][\"spoiler\"],\n",
    "        models_outputs[\"vicuna_pwt\"][\"spoiler\"],\n",
    "        models_outputs[\"opt_pwt\"][\"spoiler\"],\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 3)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 988/988 [02:57<00:00,  5.56it/s]\n"
     ]
    }
   ],
   "source": [
    "selected_spoilers = []\n",
    "for n in tqdm(range(merged_df.shape[0])):\n",
    "    spoilers = merged_df.loc[n].tolist()\n",
    "    data = [PROMPT.format(test_df.loc[n, \"question\"], i, test_df.loc[n, \"context\"]) for i in spoilers]\n",
    "    input_ids = tokenizer(data, return_tensors=\"pt\", padding=\"max_length\", truncation=True).to(\"cuda\")\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**input_ids)\n",
    "\n",
    "    selected_spoilers.append(spoilers[outputs.logits.argmax()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepare_stats(test_df, pd.DataFrame(selected_spoilers, columns=[\"spoiler\"]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select spoilers with models for specific type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_not_multi_spoilers = test_df[test_df[\"type\"]!=\"multi\"].reset_index(drop=True)\n",
    "test_multi_spoilers = test_df[test_df[\"type\"]==\"multi\"].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_vicuna_pwt_output = models_outputs[\"vicuna_pwt\"][test_df[\"type\"]==\"multi\"].reset_index(drop=True)\n",
    "multi_llama_pwt_output = models_outputs[\"llama_pwt\"][test_df[\"type\"]==\"multi\"].reset_index(drop=True)\n",
    "multi_vicuna_ppt_output = models_outputs[\"vicuna_ppt\"][test_df[\"type\"]==\"multi\"].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_merged_df = pd.DataFrame(\n",
    "    zip(\n",
    "        multi_vicuna_pwt_output[\"spoiler\"],\n",
    "        multi_vicuna_ppt_output[\"spoiler\"],\n",
    "        multi_llama_pwt_output[\"spoiler\"],\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "not_multi_vicuna_ppt_output = models_outputs[\"vicuna_ppt\"][test_df[\"type\"]!=\"multi\"].reset_index(drop=True)\n",
    "not_multi_deberta_paqott_output = models_outputs[\"deberta_paqott\"][test_df[\"type\"]!=\"multi\"].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "not_multi_merged_df = pd.DataFrame(\n",
    "    zip(\n",
    "        not_multi_vicuna_ppt_output[\"spoiler\"],\n",
    "        not_multi_deberta_paqott_output[\"spoiler\"],\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 814/814 [01:39<00:00,  8.16it/s]\n"
     ]
    }
   ],
   "source": [
    "not_multi_selected_spoilers = []\n",
    "for n in tqdm(range(not_multi_merged_df.shape[0])):\n",
    "    spoilers = not_multi_merged_df.loc[n].tolist()\n",
    "    data = [PROMPT.format(test_not_multi_spoilers.loc[n, \"question\"], i, test_not_multi_spoilers.loc[n, \"context\"]) for i in spoilers]\n",
    "    input_ids = tokenizer(data, return_tensors=\"pt\", padding=\"max_length\", truncation=True).to(\"cuda\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**input_ids)\n",
    "\n",
    "    not_multi_selected_spoilers.append(spoilers[outputs.logits.argmax()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 174/174 [00:21<00:00,  7.94it/s]\n"
     ]
    }
   ],
   "source": [
    "multi_selected_spoilers = []\n",
    "for n in tqdm(range(multi_merged_df.shape[0])):\n",
    "    spoilers = multi_merged_df.loc[n].tolist()\n",
    "    data = [PROMPT.format(test_multi_spoilers.loc[n, \"question\"], i, test_multi_spoilers.loc[n, \"context\"]) for i in spoilers]\n",
    "    input_ids = tokenizer(data, return_tensors=\"pt\", padding=\"max_length\", truncation=True).to(\"cuda\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**input_ids)\n",
    "\n",
    "    multi_selected_spoilers.append(spoilers[outputs.logits.argmax()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bleu': 0.44303314639062596,\n",
       " 'precision': 0.9096897679422549,\n",
       " 'recall': 0.913005078852418,\n",
       " 'f1': 0.9108124781354718,\n",
       " 'exact_match': 0.3208502024291498,\n",
       " 'meteor': 0.5301155410177051}"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = pd.concat(\n",
    "    [\n",
    "        pd.DataFrame(not_multi_selected_spoilers, columns=[\"spoiler\"]),\n",
    "        pd.DataFrame(multi_selected_spoilers, columns=[\"spoiler\"]),\n",
    "    ]\n",
    ").reset_index(drop=True)\n",
    "ref = pd.concat([test_not_multi_spoilers, test_multi_spoilers]).reset_index(drop=True)\n",
    "prepare_stats(ref, pred)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check best score selecting by max bleu per example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "bleu = load(\"bleu\")\n",
    "\n",
    "def calc_bleu(true_spoiler, predicted_spoiler):\n",
    "    bleu_score = []\n",
    "    for reference, hypothesis in zip(true_spoiler, predicted_spoiler):\n",
    "        try:\n",
    "            val = bleu.compute(\n",
    "                predictions=[hypothesis],\n",
    "                references=[[reference]],\n",
    "                max_order=min(4, len(reference.split(\" \"))),\n",
    "            )[\"bleu\"]\n",
    "        except ZeroDivisionError:\n",
    "            val = 0\n",
    "        bleu_score.append(val)\n",
    "    return bleu_score\n",
    "\n",
    "\n",
    "best_bleu_not_multi = pd.DataFrame(\n",
    "    zip(\n",
    "        calc_bleu(test_not_multi_spoilers[\"spoiler\"], not_multi_vicuna_ppt_output[\"spoiler\"]),\n",
    "        calc_bleu(test_not_multi_spoilers[\"spoiler\"], not_multi_deberta_paqott_output[\"spoiler\"]),\n",
    "    )\n",
    ")\n",
    "best_bleu_multi = pd.DataFrame(\n",
    "    zip(\n",
    "        calc_bleu(test_multi_spoilers[\"spoiler\"], multi_llama_pwt_output[\"spoiler\"]),\n",
    "        calc_bleu(test_multi_spoilers[\"spoiler\"], multi_vicuna_pwt_output[\"spoiler\"]),\n",
    "        calc_bleu(test_multi_spoilers[\"spoiler\"], multi_vicuna_ppt_output[\"spoiler\"]),\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0    0.429699\n",
       " 1    0.453047\n",
       " dtype: float64,\n",
       " 0    0.256311\n",
       " 1    0.255186\n",
       " 2    0.259893\n",
       " dtype: float64)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_bleu_not_multi.mean(axis=0), best_bleu_multi.mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_possible_bleu = pd.concat([best_bleu_not_multi.max(axis=1), best_bleu_multi.max(axis=1)]).mean()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Best possible bleu to achive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5363586793501569"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_possible_bleu"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifier"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"MateuszW/classifier-distilbert\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_path, num_labels=2).to(\"cuda\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT = \"For given question:\\n{question}\\nchoose what answer is better\\n\\n\\n## Answer1:\\n{ans1}\\n\\n## Answer2:\\n{ans2}\\n\\n## Context:\\n{context}\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select spoilers without using models for specific type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = pd.DataFrame(\n",
    "    zip(\n",
    "        models_outputs[\"vicuna_pwt\"][\"spoiler\"],\n",
    "        models_outputs[\"llama_pwt\"][\"spoiler\"],\n",
    "    )\n",
    ").reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 988/988 [00:21<00:00, 45.73it/s]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import itertools\n",
    "from tqdm import tqdm\n",
    "\n",
    "selected_spoilers = []\n",
    "for n in tqdm(range(merged_df.shape[0])):\n",
    "    spoilers = merged_df.loc[n].tolist()\n",
    "    score = [0] * len(spoilers)\n",
    "    for comb in itertools.combinations(range(len(spoilers)), 2):\n",
    "        if spoilers[comb[0]] == \"\":\n",
    "            score[comb[1]] += 1\n",
    "            continue\n",
    "        if spoilers[comb[1]] == \"\":\n",
    "            score[comb[0]] += 1\n",
    "            continue\n",
    "        data = [\n",
    "            PROMPT.format(question=test_df.loc[n, \"question\"], ans1=spoilers[comb[0]], ans2=spoilers[comb[1]], context=test_df.loc[n, \"context\"])\n",
    "        ]\n",
    "        input_ids = tokenizer(data, return_tensors=\"pt\", padding=\"max_length\", truncation=True).to(\"cuda\")\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**input_ids)\n",
    "        if outputs.logits.argmax() == 1:\n",
    "            score[comb[0]] += 1\n",
    "        else:\n",
    "            score[comb[1]] += 1\n",
    "\n",
    "    selected_spoilers.append(spoilers[np.argmax(score)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n"
     ]
    }
   ],
   "source": [
    "prepare_stats(test_df, pd.DataFrame(selected_spoilers, columns=[\"spoiler\"]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select spoilers with models for specific type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_not_multi_spoilers = test_df[test_df[\"type\"]!=\"multi\"].reset_index(drop=True)\n",
    "test_multi_spoilers = test_df[test_df[\"type\"]==\"multi\"].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "not_multi_merged_df = pd.DataFrame(\n",
    "    zip(\n",
    "        not_multi_vicuna_ppt_output[\"spoiler\"],\n",
    "        not_multi_deberta_paqott_output[\"spoiler\"],\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_merged_df = pd.DataFrame(\n",
    "    zip(\n",
    "        multi_vicuna_pwt_output[\"spoiler\"],\n",
    "        multi_vicuna_ppt_output[\"spoiler\"],\n",
    "        multi_llama_pwt_output[\"spoiler\"],\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 814/814 [00:17<00:00, 47.46it/s]\n"
     ]
    }
   ],
   "source": [
    "not_multi_selected_spoilers = []\n",
    "for n in tqdm(range(not_multi_merged_df.shape[0])):\n",
    "    spoilers = not_multi_merged_df.loc[n].tolist()\n",
    "    if spoilers[0] == \"\":\n",
    "        not_multi_selected_spoilers.append(spoilers[1])\n",
    "        continue\n",
    "    if spoilers[1] == \"\":\n",
    "        not_multi_selected_spoilers.append(spoilers[0])\n",
    "        continue\n",
    "    data = [PROMPT.format(question=test_not_multi_spoilers.loc[n, \"question\"], ans1=spoilers[0], ans2=spoilers[1], context=test_not_multi_spoilers.loc[n, \"context\"])]\n",
    "    input_ids = tokenizer(data, return_tensors=\"pt\", padding=\"max_length\", truncation=True).to(\"cuda\")\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**input_ids)\n",
    "\n",
    "    not_multi_selected_spoilers.append(spoilers[1 - outputs.logits.argmax()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 174/174 [00:03<00:00, 44.88it/s]\n"
     ]
    }
   ],
   "source": [
    "multi_selected_spoilers = []\n",
    "for n in tqdm(range(multi_merged_df.shape[0])):\n",
    "    spoilers = multi_merged_df.loc[n].tolist()\n",
    "    score = [0] * len(spoilers)\n",
    "    for comb in itertools.combinations(range(len(spoilers)), 2):\n",
    "        if spoilers[comb[0]] == \"\":\n",
    "            score[comb[1]] += 1\n",
    "            continue\n",
    "        if spoilers[comb[1]] == \"\":\n",
    "            score[comb[0]] += 1\n",
    "            continue\n",
    "        data = [\n",
    "            PROMPT.format(\n",
    "                question=test_multi_spoilers.loc[n, \"question\"], ans1=spoilers[comb[0]], ans2=spoilers[comb[1]], context=test_multi_spoilers.loc[n, \"context\"]\n",
    "            )\n",
    "        ]\n",
    "        input_ids = tokenizer(data, return_tensors=\"pt\", padding=\"max_length\", truncation=True).to(\"cuda\")\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**input_ids)\n",
    "        if outputs.logits.argmax() == 1:\n",
    "            score[comb[0]] += 1\n",
    "        else:\n",
    "            score[comb[1]] += 1\n",
    "\n",
    "    multi_selected_spoilers.append(spoilers[np.argmax(score)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bleu': 0.43455372191988867,\n",
       " 'precision': 0.9114617712584584,\n",
       " 'recall': 0.9109391218978866,\n",
       " 'f1': 0.9106571890323268,\n",
       " 'exact_match': 0.31275303643724695,\n",
       " 'meteor': 0.5198940468784699}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = pd.concat(\n",
    "    [\n",
    "        pd.DataFrame(not_multi_selected_spoilers, columns=[\"spoiler\"]),\n",
    "        pd.DataFrame(multi_selected_spoilers, columns=[\"spoiler\"]),\n",
    "    ]\n",
    ").reset_index(drop=True)\n",
    "ref = pd.concat([test_not_multi_spoilers, test_multi_spoilers]).reset_index(drop=True)\n",
    "prepare_stats(ref, pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
